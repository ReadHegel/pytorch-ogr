{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de00992",
   "metadata": {},
   "source": [
    "## Ten notebook zawiera serie por贸wna dOGR nonlinear clipping z Adam na kilku datasetach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d8cc3",
   "metadata": {},
   "source": [
    "Por贸wnujemy zbie偶no na zbiorze treningowym na przestrzeni 50 epoch, czas trwania caego treningu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab4b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80312d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In module products __package__, __name__ == tests tests.main\n",
      "In module products __package__, __name__ == tests tests.trainloop\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import optim\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "# Zaimportuj reu偶ywalne komponenty z Twoich skrypt贸w\n",
    "# Upewnij si, 偶e ten notebook jest w g贸wnym folderze projektu\n",
    "from tests.main import run, net_dict\n",
    "from tests.datamodule import MNISTDataModule, CIFAR10DataModule, FashionMNISTDataModule\n",
    "from src.optim.dOGR import dOGR\n",
    "from tests.nets import FC, LeNet\n",
    "\n",
    "# --- G贸wne Ustawienia Eksperymentu ---\n",
    "LOGGING_DIR = Path(\"logs\")\n",
    "MAX_EPOCHS = 6\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sownik z DataModules dla atwej iteracji\n",
    "# datamodules = {\n",
    "#     \"MNIST\": MNISTDataModule,\n",
    "#     \"FashionMNIST\": FashionMNISTDataModule,\n",
    "#     \"CIFAR10\": CIFAR10DataModule\n",
    "# }\n",
    "datamodules = {\n",
    "    \"CIFAR10\": CIFAR10DataModule\n",
    "}\n",
    "\n",
    "# 1. Konfiguracja dla Twojego najlepiej dostrojonego dOGR\n",
    "dogr_config = {\n",
    "    \"opt\": dOGR,\n",
    "    \"args\": {\n",
    "        \"lr\": 1e-3,\n",
    "        \"nonlinear_clipping\": True,\n",
    "        \"p_norm\": 0.57,\n",
    "        \"p_eps\": 10.77,\n",
    "        \"beta\": 0.1,\n",
    "        \"trust_factor\": 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. Siatka parametr贸w do przeszukania dla Adama (Grid Search)\n",
    "adam_grid_search = {\n",
    "    \"opt\": optim.Adam,\n",
    "    \"grid\": {\n",
    "        \"lr\": [1e-2, 1e-3, 1e-4],\n",
    "        \"betas\": [(0.9, 0.999), (0.9, 0.99)],\n",
    "        \"weight_decay\": [0, 1e-4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# adam_grid_search = {\n",
    "#     \"opt\": optim.Adam,\n",
    "#     \"grid\": {\n",
    "#         \"lr\": [1e-2, 3e-3, 1e-3, 3e-4, 1e-4],  # 5 values\n",
    "#         \"betas\": [(0.9, 0.999), (0.9, 0.99), (0.8, 0.999), (0.95, 0.999)], # 4 values\n",
    "#         \"weight_decay\": [0, 1e-5, 1e-4, 1e-3], # 4 values\n",
    "#         \"eps\": [1e-8, 1e-7] # 2 values\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# net_dict = {\n",
    "#     \"FC\": FC,\n",
    "#     \"LeNet\": LeNet\n",
    "# }\n",
    "net_dict = {\n",
    "    \"LeNet\": LeNet,\n",
    "}\n",
    "\n",
    "# Sownik do przechowywania najlepszych wynik贸w dla ka偶dego datasetu\n",
    "best_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b6f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================= ROZPOCZYNANIE TESTW DLA ARCHITEKTURY: FC =========================\n",
      "\n",
      "-------------------- ZBIR DANYCH: MNIST --------------------\n",
      "\n",
      "--- Uruchamianie dOGR (wersja 'dogr_tuned') ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/dogr_tuned exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/dogr_tuned/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:05<00:00, 40.99it/s, v_num=uned]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:05<00:00, 40.87it/s, v_num=uned]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 63.74it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          95.47000122070312\n",
      "        test_loss           0.14548268914222717\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Uruchamianie Grid Search dla Adama ---\n",
      "[1/12] Adam z parametrami: {'lr': 0.01, 'betas': (0.9, 0.999), 'weight_decay': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:04<00:00, 44.41it/s, v_num=wd=0]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:04<00:00, 44.31it/s, v_num=wd=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 65.13it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          96.8499984741211\n",
      "        test_loss           0.12885479629039764\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/12] Adam z parametrami: {'lr': 0.01, 'betas': (0.9, 0.999), 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:04<00:00, 43.48it/s, v_num=0001]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:04<00:00, 43.37it/s, v_num=0001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 65.42it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          95.80000305175781\n",
      "        test_loss           0.15257716178894043\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/12] Adam z parametrami: {'lr': 0.01, 'betas': (0.9, 0.99), 'weight_decay': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.01_wd=0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.01_wd=0/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:04<00:00, 43.12it/s, v_num=wd=0]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:04<00:00, 43.02it/s, v_num=wd=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 61.56it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          96.30999755859375\n",
      "        test_loss           0.1575193554162979\n",
      "\n",
      "[4/12] Adam z parametrami: {'lr': 0.01, 'betas': (0.9, 0.99), 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.01_wd=0.0001 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.01_wd=0.0001/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:05<00:00, 42.08it/s, v_num=0001]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:05<00:00, 41.99it/s, v_num=0001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 63.77it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          95.94000244140625\n",
      "        test_loss           0.1444736272096634\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/12] Adam z parametrami: {'lr': 0.001, 'betas': (0.9, 0.999), 'weight_decay': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:05<00:00, 39.02it/s, v_num=wd=0]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:05<00:00, 38.92it/s, v_num=wd=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 59.02it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          96.91000366210938\n",
      "        test_loss           0.10021103918552399\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/12] Adam z parametrami: {'lr': 0.001, 'betas': (0.9, 0.999), 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:05<00:00, 39.37it/s, v_num=0001]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:05<00:00, 39.28it/s, v_num=0001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 63.55it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          96.83999633789062\n",
      "        test_loss           0.10158338397741318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/12] Adam z parametrami: {'lr': 0.001, 'betas': (0.9, 0.99), 'weight_decay': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.001_wd=0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.001_wd=0/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:05<00:00, 39.41it/s, v_num=wd=0]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:05<00:00, 39.33it/s, v_num=wd=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 67.47it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          97.08999633789062\n",
      "        test_loss           0.1020994558930397\n",
      "\n",
      "[8/12] Adam z parametrami: {'lr': 0.001, 'betas': (0.9, 0.99), 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.001_wd=0.0001 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.001_wd=0.0001/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:04<00:00, 43.54it/s, v_num=0001]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:04<00:00, 43.43it/s, v_num=0001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 65.70it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          97.05999755859375\n",
      "        test_loss           0.09953679889440536\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/12] Adam z parametrami: {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:06<00:00, 31.18it/s, v_num=wd=0]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:06<00:00, 31.12it/s, v_num=wd=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 51.29it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          92.98999786376953\n",
      "        test_loss           0.24515141546726227\n",
      "\n",
      "[10/12] Adam z parametrami: {'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:06<00:00, 32.22it/s, v_num=0001]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:06<00:00, 32.17it/s, v_num=0001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 50.76it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          92.97000122070312\n",
      "        test_loss           0.24489855766296387\n",
      "\n",
      "[11/12] Adam z parametrami: {'lr': 0.0001, 'betas': (0.9, 0.99), 'weight_decay': 0}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.0001_wd=0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.0001_wd=0/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:06<00:00, 35.03it/s, v_num=wd=0]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 215/215 [00:06<00:00, 34.96it/s, v_num=wd=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 40/40 [00:00<00:00, 55.63it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          92.97000122070312\n",
      "        test_loss           0.2455274611711502\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/12] Adam z parametrami: {'lr': 0.0001, 'betas': (0.9, 0.99), 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.0001_wd=0.0001 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/mikolaj/work/pytorch-ogr/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/mikolaj/work/pytorch-ogr/logs/dOGR_vs_Adam_FC_MNIST/Adam_lr=0.0001_wd=0.0001/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | net  | FC   | 55.1 K | train\n",
      "--------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 215/215 [00:05<00:00, 40.64it/s, v_num=0001]     "
     ]
    }
   ],
   "source": [
    "def run_experiments_on_all_architectures():\n",
    "    \"\"\"\n",
    "    Uruchamia serie eksperyment贸w dla wszystkich zdefiniowanych architektur sieci\n",
    "    i wszystkich zdefiniowanych zbior贸w danych.\n",
    "    \"\"\"\n",
    "    # ZMIANA: Zewntrzna ptla iterujca po sieciach (\"FC\", \"LeNet\")\n",
    "    for net_name in net_dict.keys():\n",
    "        print(f\"\\n{'='*25} ROZPOCZYNANIE TESTW DLA ARCHITEKTURY: {net_name.upper()} {'='*25}\")\n",
    "        \n",
    "        # Wewntrzna ptla iterujca po zbiorach danych\n",
    "        for dataset_name, datamodule_class in datamodules.items():\n",
    "            print(f\"\\n{'--'*10} ZBIR DANYCH: {dataset_name.upper()} {'--'*10}\")\n",
    "            \n",
    "            # Tworzymy DataModule, aby uzyska wymiary dla modelu\n",
    "            dm = datamodule_class(batch_size=BATCH_SIZE)\n",
    "            \n",
    "            # Nazwa eksperymentu zawiera teraz nazw sieci i datasetu\n",
    "            experiment_name = f\"dOGR_vs_Adam_{net_name}_{dataset_name}\"\n",
    "            \n",
    "            # --- Uruchomienie eksperymentu dla dOGR ---\n",
    "            print(f\"\\n--- Uruchamianie dOGR (wersja 'dogr_tuned') ---\")\n",
    "            torch.manual_seed(42)\n",
    "            # Dynamiczne tworzenie sieci z poprawnymi wymiarami\n",
    "            net_dogr = net_dict[net_name](input_dims=dm.dims, num_classes=dm.num_classes)\n",
    "            optimizer_dogr = dogr_config[\"opt\"](net_dogr.parameters(), **dogr_config[\"args\"])\n",
    "            run(\n",
    "                net=net_dogr, optimizer=optimizer_dogr, name=experiment_name, version=\"dogr_tuned\",\n",
    "                datamodule=dm, max_epochs=MAX_EPOCHS, batch_size=BATCH_SIZE\n",
    "            )\n",
    "\n",
    "            # --- Uruchomienie Grid Search dla Adama ---\n",
    "            print(f\"\\n--- Uruchamianie Grid Search dla Adama ---\")\n",
    "            best_adam_run = {\"accuracy\": -1, \"version\": None}\n",
    "            \n",
    "            keys, values = zip(*adam_grid_search[\"grid\"].items())\n",
    "            param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "            adam_runs_results = []\n",
    "\n",
    "            for i, params in enumerate(param_combinations):\n",
    "                version_name = f\"Adam_lr={params['lr']}_wd={params.get('weight_decay', 0)}\"\n",
    "                print(f\"[{i+1}/{len(param_combinations)}] Adam z parametrami: {params}\")\n",
    "                \n",
    "                torch.manual_seed(42)\n",
    "                # Dynamiczne tworzenie sieci z poprawnymi wymiarami\n",
    "                net_adam = net_dict[net_name](input_dims=dm.dims, num_classes=dm.num_classes)\n",
    "                optimizer_adam = adam_grid_search[\"opt\"](net_adam.parameters(), **params)\n",
    "                \n",
    "                run(\n",
    "                    net=net_adam, optimizer=optimizer_adam, name=experiment_name, version=version_name,\n",
    "                    datamodule=dm, max_epochs=MAX_EPOCHS, batch_size=BATCH_SIZE\n",
    "                )\n",
    "                \n",
    "                log_path = LOGGING_DIR / experiment_name / version_name / \"metrics.csv\"\n",
    "                if log_path.exists():\n",
    "                    df = pd.read_csv(log_path)\n",
    "                    \n",
    "                    # Filtrujemy dane, aby usun ewentualne brakujce wartoci\n",
    "                    loss_df = df.dropna(subset=['epoch', 'train_loss'])\n",
    "                    \n",
    "                    if not loss_df.empty:\n",
    "                        # Znajd藕 numer ostatniej epoki\n",
    "                        last_epoch = loss_df['epoch'].max()\n",
    "                        \n",
    "                        # Wybierz dane z 5 ostatnich epok\n",
    "                        last_5_epochs_df = loss_df[loss_df['epoch'] > last_epoch - 5]\n",
    "                        \n",
    "                        if not last_5_epochs_df.empty:\n",
    "                            # Oblicz redni strat i u偶yj jej jako metryki\n",
    "                            avg_final_loss = last_5_epochs_df['train_loss'].mean()\n",
    "                            adam_runs_results.append({'version': version_name, 'metric': avg_final_loss})\n",
    "                # ----------------------------------------------------------------------\n",
    "\n",
    "            # ZMIANA: Sortujemy po nowej metryce (im ni偶sza strata, tym lepiej)\n",
    "            adam_runs_results.sort(key=lambda x: x['metric'])\n",
    "            top_5_adam = adam_runs_results[:5]\n",
    "            \n",
    "            best_results[f\"{net_name}_{dataset_name}\"] = {\n",
    "                \"dogr_version\": \"dogr_tuned\", \n",
    "                \"adam_top_5\": top_5_adam\n",
    "            }\n",
    "            print(f\"\\nNajlepsze 5 wersji Adama dla {net_name} na {dataset_name} (wg. kocowej straty):\")\n",
    "            for run_result in top_5_adam:\n",
    "                print(f\"  - {run_result['version']} (r. strata: {run_result['metric']:.6f})\")\n",
    "# Uruchomienie caego procesu\n",
    "run_experiments_on_all_architectures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_results():\n",
    "    \"\"\"\n",
    "    Generuje osobny wykres por贸wnawczy dla ka偶dej kombinacji sie-dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generowanie wykres贸w por贸wnawczych ---\")\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "    # ZMIANA: G贸wna ptla iteruje teraz po zapisanych wynikach\n",
    "    for result_key, result_data in best_results.items():\n",
    "        \n",
    "        # Tworzymy nowy, osobny wykres dla ka偶dej kombinacji\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        net_name, dataset_name = result_key.split('_', 1)\n",
    "        experiment_name = f\"dOGR_vs_Adam_{net_name}_{dataset_name}\"\n",
    "        \n",
    "        fig.suptitle(f\"Por贸wnanie na: {dataset_name} | Architektura: {net_name} ({MAX_EPOCHS} epok)\", fontsize=16)\n",
    "        \n",
    "        # 1. Wykres dla dOGR\n",
    "        dogr_version = result_data[\"dogr_version\"]\n",
    "        dogr_log_path = LOGGING_DIR / experiment_name / dogr_version / \"metrics.csv\"\n",
    "        if dogr_log_path.exists():\n",
    "            df_dogr = pd.read_csv(dogr_log_path)\n",
    "            loss_dogr = df_dogr.dropna(subset=['train_loss']).groupby('epoch')['train_loss'].mean()\n",
    "            ax.plot(loss_dogr.index, loss_dogr.values, \"-\", label=f\"dOGR (dostrojony)\", linewidth=2.5, color='red')\n",
    "        \n",
    "        # 2. Wykresy dla 5 najlepszych Adam贸w\n",
    "        top_5_adam = result_data[\"adam_top_5\"]\n",
    "        for adam_run in top_5_adam:\n",
    "            adam_version = adam_run['version']\n",
    "            adam_log_path = LOGGING_DIR / experiment_name / adam_version / \"metrics.csv\"\n",
    "            if adam_log_path.exists():\n",
    "                df_adam = pd.read_csv(adam_log_path)\n",
    "                loss_adam = df_adam.dropna(subset=['train_loss']).groupby('epoch')['train_loss'].mean()\n",
    "                # U偶ywamy etykiety, aby pokaza parametry i wynik\n",
    "                label = f\"{adam_version} (acc: {adam_run['accuracy']:.2f}%)\"\n",
    "                ax.plot(loss_adam.index, loss_adam.values, \"-\", label=label, alpha=0.7)\n",
    "\n",
    "        ax.set_xlabel(\"Epoka\")\n",
    "        ax.set_ylabel(\"rednia strata treningowa (skala log)\")\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(True, which='both', linestyle='--')\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1be79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
